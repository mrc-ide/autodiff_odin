@inproceedings{Chen2018,
abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
author = {Chen, Ricky T.Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
pages = {6571--6583},
publisher = {Neural information processing systems foundation},
title = {{Neural ordinary differential equations}},
volume = {2018-December},
year = {2018}
}

@book{AndreasGriewank2008,
abstract = {Algorithmic, or automatic, differentiation (AD) is a growing area of theoretical research and software development concerned with the accurate and efficient evaluation of derivatives for function evaluations given as computer programs. The resulting derivative values are useful for all scientific computations that are based on linear, quadratic, or higher order approximations to nonlinear scalar or vector functions. This second edition covers recent developments in applications and theory, including an elegant NP completeness argument and an introduction to scarcity. There is also added material on checkpointing and iterative differentiation. To improve readability the more detailed analysis of memory and complexity bounds has been relegated to separate, optional chapters. The book consists of: a stand-alone introduction to the fundamentals of AD and its software; a thorough treatment of methods for sparse problems; and final chapters on program-reversal schedules, higher derivatives, nonsmooth problems and iterative processes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.05767},
author = {{Andreas Griewank}, Andrea Walther},
booktitle = {SIAM - Society for Industrial and Applied Mathematic},
eprint = {arXiv:1502.05767},
isbn = {0898714516 (pbk.)},
issn = {0949-1775},
keywords = {Algorithmic Differentiation,Automatic Differentiation},
pages = {438},
pmid = {8384989},
title = {{Evaluating Derivatives Principles and Techniques of Algorithmic Differentiation}},
volume = {2},
year = {2008}
}

