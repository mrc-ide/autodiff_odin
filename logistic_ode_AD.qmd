---
title: "`Reverse Automatic Differentiation for odes with odin`"
author: "Marc Baguelin"
format:
  revealjs:
    slide-number: c/t
    footer: "[AutoDiff with odin (ODE systems)](https://www.imperial.ac.uk/people/m.baguelin)"
---

# Principles

## General {.smaller}

![](images/ode_AD_chen.png){.absolute top=180 left=500 width="652" height="450"}

:::: {.columns}

::: {.column width="30%"}

- Adjoints
$$a(t) = \frac{\partial L}{\partial z(t)}$$
- "Continuous" chain rule
$$\frac{da(t)}{dt}=-a(t)^{\intercal} \frac{\partial f(z(t),t,\theta)}{\partial z}$$ 
- Parameter "continuous" accumulation $$\frac{dL}{d \theta} = \int_{t_{1}}^{t_{0}} a(t)^{\intercal} \frac{\partial f(z(t),t,\theta)}{\partial \theta}dt$$  
:::

::: {.column width="20%"}
:::

::: {.column width="50%"}
[Chen et al. "Neural Ordinary Differential Equations", *NeurIPS 2018*](https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf)
:::

::::


## Comparison with "discrete" AD {.smaller}

very similar algorithm

* Step 1: run the system forward until the last data point
* Step 2: compute the contribution of data point to adjoints
* Step 3: compute the backward system (including state, adjoint state and adjoint parameters) until previous data point or initial time
* Step 4: if not initial time go to step 2, if initial go to step 5
* Step 5: add the contribution of initial conditions to adjoint of parameters
* Step 6: read gradient


# Worked example: logistic function

## About the logistic (growth) function

- Simple population growth model with saturation
- 1-D ODE
- Three parameters 
- Known analytical solution
- Non-linear

![](images/Pierre_Francois_Verhulst.jpeg){.absolute top=250 left=800 width="120" height="181"}

## ODE form {.smaller}

The ODE form of the logistic function is:

$$
\begin{cases}
\begin{align*}
\frac{dN}{dt} &= rN\left(1 - \frac{N}{K}\right) \\
N(0) &= N_{0}
\end{align*}
\end{cases}
$$

where:

- $K$ is the carrying capacity or maximum value the function can reach
- $r$ is the growth rate or steepness of the curve
- $N_{0}$ is the initial condition of the ODE 

## Equation form and observation model

- The equation form of the logistic function is given by:
$$N(t) = \frac{K}{1 + \left ( \frac{K}{N_{0}}-1 \right ) e^{-rt}}$$
- We can also add an observation model
$$N^{obs}_{t} \sim \mathcal{N}(N(t), \sigma),\; t \in T_{obs}=\{ t_{1},...,t_{n_{obs}} \}$$

## odin model

very compact and easy to read
```{r}
r_output <- function(path, highlight = NULL) {
  if (is.null(highlight)) {
    prefix <- "```r"
  } else {
    prefix <- sprintf('```{.r code-line-numbers="%s"}', highlight)
  }
  writeLines(c(prefix, readLines(path), "```"))
}
set.seed(1) # always the same
knitr::knit_hooks$set(small_margins = function(before, options, envir) {
  if (before) {
    par(mar = c(4, 4, .1, .1))
  }
})
```

```{r}
#| results: "asis"
r_output("models/logistic_growth_normal_obs.R")
```

The model (including noisy observation) has 4 parameters $r$, $K$, $N_{0}$ and $\sigma$ the s.d. of the observation process

## Logistic function plot

We can use odin.dust to integrate the ODE:

```{r}
#| echo: TRUE
generator <- odin.dust::odin_dust("models/logistic_growth_normal_obs.R")
```

## Logistic function plot

```{r}
#default parameters
N0 <- 2
K <- 100
r <- 1

mod <- generator$new(pars=list(r=r, N0=N0, K=K, sd_noise=5), time = 0, n_particles = 1)

n_obs <- 20
t_obs <- seq(1, n_obs)

#Plot the trajectory
tt <- seq(0, 25, length.out = 101)
y <- mod$simulate(tt)[,1,]
plot(tt, y, xlab = "t", ylab = "N(t)", main = "", type="l", ylim= c(0,120), lty=1)
```

## Logistic function plot + observations {visibility="uncounted"}

```{r plot-observations}
n_obs <- 20
t_obs <- seq(1, n_obs)

#computes the mid-point time (t0 such that N(t0)=K/2)
t0 <- log(K/N0-1)/r

#plot the analytical solution of the plotted model
N_obs <- K/(1+exp(-r*(t_obs-t0)))
#points(t_obs, N_obs)

#generates noisy observations/data
sd_noise <- 5
d_df <- data.frame(time = t_obs,
                   observed = rnorm(N_obs,N_obs, sd_noise))
plot(tt, y, xlab = "t", ylab = "N(t)", main = "", type="l", ylim= c(0,120), lty=3)
points(t_obs, N_obs, pch=19, col="grey")
points(t_obs, d_df$observed, pch=19, col="black")
for(i in seq_along(t_obs)){
  lines(c(t_obs[i],t_obs[i]), c(N_obs[i],d_df$observed[i]))
}
```

## Analytical gradient (1) {.smaller}

The log-likelihood of the model is:
$$\log(L) = \sum_{t \in T_{obs}} \log \left( f(N^{obs}_{t}, N(t), \sigma ) \right )$$
with

- $N^{obs}_{t},t \in T_{obs}$ the observations
- $f$ the pdf of the Normal distribution

$\frac{\partial \log(L)}{\partial \sigma}$ can be calculated relatively easily and 
for partial derivatives wrt parameter $X \in \{ r,K,N_{0} \}$, we can use:
$$\frac{\partial \log(L)}{\partial X} = \sum_{t \in T_{obs}} \frac{\partial \log (f)}{\partial N} \bigg|_{N^{obs}_{t},N(t), \sigma} \frac{\partial N}{\partial X} \bigg|_{r,K,N_{0},t}$$

## Analytical gradient (2) {.smaller}

The log of the normal pdf is
$$ \log (f) = -\log \sigma - \frac{1}{2} \log (2\pi) - \frac{1}{2 \sigma^2} ( N^{obs}-N )^2$$
So we have

$$\frac{\partial \log(L)} {\partial \sigma} = \sum_{t \in T_{obs}} \left ( -\frac{1}{\sigma} +\frac{1}{\sigma^3}(N^{obs}_{t}-N)^2 \right ) $$

and we also get 
$$\frac{\partial \log (f)}{\partial N} \bigg|_{N(t), \sigma} = \frac{N^{obs}-N}{\sigma^2}$$ 

## Analytical gradient (3) {.smaller}

So for $X \in \{ r,K,N_{0} \}$ we get
$$\frac{\partial \log(L)}{\partial X} = \sum_{t \in T_{obs}} \frac{N^{obs}_{t}-N}{\sigma^2} \frac{\partial N}{\partial X} \bigg|_{r,K,N_{0},t}$$

To which we need to plug the partial derivatives with respect to $r$, $K$, $N_{0}$

$$
\begin{cases}
\begin{align*}
\frac{{\partial N(t)}}{{\partial r}} & = \frac{{K \cdot t \cdot (K - N_0) \cdot e^{-rt}}}{{N_0 \cdot \left(1 + \left ( \frac{K}{N_{0}}-1 \right ) \cdot e^{-rt}\right)^2}} \\
\frac{{\partial N(t)}}{{\partial K}}  & = \frac{1}{{\left(1 + \left ( \frac{K}{N_{0}}-1 \right ) \cdot e^{-rt}\right)^2}} \\
\frac{{\partial N(t)}}{{\partial N_0}} & = \frac{{K^2 \cdot e^{-rt}}}{{N_0^2 \cdot \left(1 + \left(\frac{{K}}{{N_0}}-1\right) \cdot e^{-rt}\right)^2}}
\end{align*}
\end{cases}
$$

## We can write R code for these

```{r}
#| echo: TRUE
# Gradient function with respect to K
partial_derivative_K <- function(K, N0, r, t) {
  gradient <- 1 / (1 + ((K - N0) / N0) * exp(-r * t))^2
  return(gradient)
}

# Gradient function with respect to N0
partial_derivative_N0 <- function(K, N0, r, t) {
  gradient <- (K^2 / N0^2) * exp(-r * t) / (1 + ((K - N0) / N0) * exp(-r * t))^2
  return(gradient)
}

# Gradient function with respect to r
partial_derivative_r <- function(K, N0, r, t) {
  gradient <- (K * t * (K - N0) * exp(-r * t)) / (N0 * (1 + ((K - N0) / N0) * exp(-r * t))^2)
  return(gradient)
}
```

## and for the full likelihood gradient

```{r}
#| echo: TRUE
likelihood_gradient <- function(K, N0, r, sigma, N_obs, t_obs){
  t0 <- log(K/N0-1)/r
  g_LL_K <- 0
  g_LL_N0 <- 0
  g_LL_r <- 0
  g_LL_sigma <- 0
  for(i in seq_along(t_obs)){
    N_t <- K/(1+exp(-r*(t_obs[i]-t0)))
    
    #Calculate the simpler PD wrt sigma
    g_LL_sigma <- g_LL_sigma - 1/sigma + 1/sigma^3*(N_obs[i]- N_t)^2
    
    #Normal pdf contribution
    dlogf_dN <- (N_obs[i]- N_t)/sigma^2
    g_LL_K <- g_LL_K + dlogf_dN * partial_derivative_K(K, N0, r, t_obs[i])
    g_LL_N0 <- g_LL_N0 + dlogf_dN * partial_derivative_N0(K, N0, r, t_obs[i])
    g_LL_r <- g_LL_r + dlogf_dN * partial_derivative_r(K, N0, r, t_obs[i])
  
  }
  return(list(sigma = g_LL_sigma, N0 = g_LL_N0, K = g_LL_K, r = g_LL_r))
}
```

## Analytical gradient results

We then get painfully the gradient of the log-likelihood of our model:
```{r}
#| echo: TRUE
gradient_analytical <- likelihood_gradient(K, N0, r, sd_noise, d_df$observed, t_obs)
gradient_analytical
```

Note that the trick only worked because we had an exact solution of the ode!

## Building the dependency graph

First let's build the graph of dependencies in the main odin model

```{r}
#| echo: TRUE
parsed_model <- jsonlite::fromJSON(odin::odin_parse("models/logistic_growth_normal_obs.R"))
construct_param_tree <- function(parsed_model){
  parameter_graph <- NULL
  for(n in seq_along(parsed_model$equations$name)){
    if(!is.null(parsed_model$equations$depends$variables[n][[1]])){
      for(p in eval(parsed_model$equations$depends$variables[n][[1]]))
        parameter_graph <- rbind(parameter_graph,
                                 c(p,parsed_model$equations$name[n]))
    }
  }
  igraph::graph_from_edgelist(parameter_graph)
}
param_graph <- construct_param_tree(parsed_model)
```

## Dependency graph

```{r}
#| echo: TRUE
igraph::print_all(param_graph)
```

## Autodiff gradient (1)

**Idea:** After a pass forward, we are looping through the observation points backward in time going from one state to the other using a system of ODEs containing:

(1) the states;
(2) the ajoints of the states;
(3) the adjoints of parameters (apart from compare or initial conditions ones)

## Autodiff gradient (1)

- We run the ODEs forward to get the end point:
```{r}
#| echo: TRUE
#Run system until last observation
last_obs <- length(t_obs)
mod$initialize(pars=list(r=1, sd_noise=5), time = 0, n_particles = 1)
y_end <- mod$run(last_obs)
```



## Autodiff gradient (2) {.smaller}

- We then need to run backward the state equations
- Running backward in time = change of variable $\rightarrow s = t_{end}-t$
- The ODE system $\frac{dz(t)}{dt}=f(z(t),t,\theta)$ then becomes
$$
\begin{align*}
\frac{dz(s)}{ds} &= \frac{dz(t)}{dt}\frac{dt}{ds} \\
 &= -f(z(t_{end}-s),t_{end}-s,\theta)
\end{align*}
$$

:::: {.columns}

::: {.column width="50%"}

Forward version
```{r, eval=FALSE}
#| echo: TRUE
deriv(N) <- r * N * (1 - N / K)
initial(N) <- N0
```
:::

::: {.column width="50%"}

Backward version
```{r, eval=FALSE}
#| echo: TRUE
deriv(N) <- r * N * (N / K - 1)
initial(N) <- N_end
```
:::

::::


## Compiling the reverse-with-adjoints model

```{r}
#reversing the ode's
generator_reverse <- odin.dust::odin_dust("models/reverse_AD_logistic.R")

N_curr <- N_obs[last_obs]
t_curr <- t_obs[last_obs]
adj_N_curr <- 0
adj_K_curr <- 0
adj_r_curr <- 0
adj_sigma_curr <- 0

reverse_mod <- generator_reverse$new(pars= list(r=1,
                                                N_end=N_curr,
                                                t_end=t_curr,
                                                adj_N_end=adj_N_curr,
                                                adj_K_end=0,
                                                adj_r_end=0), time=0, n_particles = 1)

contribution_data <- function(observed, model, sd){
  (observed-model)/sd^2
}

contribution_data_sigma <- function(observed, model, sd){
  -1/sd + (observed-model)^2/sd^3
}

for(i in seq_along(t_obs)){
  y_curr <- d_df$observed[n_obs-i+1]
  adj_N_curr <- adj_N_curr + contribution_data(N_curr, y_curr, sd_noise)
  adj_sigma_curr <- adj_sigma_curr + contribution_data_sigma(N_curr, y_curr, sd_noise)
  reverse_mod$initialize(pars= list(r=1,
                                    N_end=N_curr,
                                    t_end=t_curr,
                                    adj_N_end=adj_N_curr,
                                    adj_K_end=adj_K_curr,
                                    adj_r_end=adj_r_curr), time=0, n_particles = 1)
  
  flow_time <- if(n_obs>i) t_curr-t_obs[n_obs-i] else t_curr
  reverse_mod$run(flow_time)

  N_curr <- reverse_mod$state()[reverse_mod$info()$index$N]
  t_curr <- t_obs[n_obs-i]
  adj_N_curr <- reverse_mod$state()[reverse_mod$info()$index$adj_N]
  adj_K_curr <- reverse_mod$state()[reverse_mod$info()$index$adj_K]
  adj_r_curr <- reverse_mod$state()[reverse_mod$info()$index$adj_r]
}
```

## Autodiff gradient (3)
```{r}
#| echo: TRUE
gradient_AD <- c(
adj_sigma_curr,
reverse_mod$state()[reverse_mod$info()$index$adj_N],
reverse_mod$state()[reverse_mod$info()$index$adj_K],
reverse_mod$state()[reverse_mod$info()$index$adj_r])
name_var <- c("sigma","N0","K","r")
```

## Numerical differentiation gradient

```{r numerical differentiation}
#| echo: TRUE

#TODO: do we need this if data is used in the PF?
d <- dust::dust_data(d_df)
mod$set_data(d)

#creates data for pf
d_df$t <- d_df$time #rename to respect convention that time should not be called time ;)
pf_data <- mcstate::particle_filter_data(d_df, "t", rate=NULL, initial_time = 0)

#creating the filter
filter <- mcstate::particle_filter$new(data=pf_data, generator, n_particles = 1, compare = NULL)
h <- 1e-6
ND_sigma <- (filter$run(pars=list(r=r, N0=N0, K=K, sd_noise=5+h))-filter$run(pars=list(r=r, N0=N0, K=K, sd_noise=5)))/h
ND_N <- (filter$run(pars=list(r=r, N0=N0+h, K=K, sd_noise=5))-filter$run(pars=list(r=r, N0=N0, K=K, sd_noise=5)))/h
ND_K <- (filter$run(pars=list(r=r, N0=N0, K=K+h, sd_noise=5))-filter$run(pars=list(r=r, N0=N0, K=K, sd_noise=5)))/h
ND_r <- (filter$run(pars=list(r=r+h, N0=N0, K=K, sd_noise=5))-filter$run(pars=list(r=r, N0=N0, K=K, sd_noise=5)))/h

gradient_ND <- c(ND_sigma,ND_N,ND_K,ND_r)
gradient_ND
```

## Comparison of the different methods

```{r}
knitr::kable(data.frame(Variable = name_var,
                        AD = gradient_AD,
                        analytical = unlist(gradient_analytical),
                        ND = gradient_ND))
```

## Running Gradient descent


<!--  LocalWords:  revealjs sprintf writeLines readLines 
 -->
