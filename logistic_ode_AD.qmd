---
title: "`Reverse Automatic Differentiation for odes with odin`"
author: "Marc Baguelin"
format:
  revealjs:
    slide-number: c/t
---

# Automatic Differentiation

## General principles {.smaller}

- Based on Chen et al. "Neural Ordinary Differential Equations", NeurIPS 2018
![](images/ode_AD_chen.png){.absolute top=200 left=0 width="652" height="450"}

## Comparison with "discrete" AD {.smaller}

very similar algorithm

* Step 1: run the system forward until the last data point
* Step 2: compute the contribution of data point to adjoints
* Step 3: compute the backward system (including state, adjoint state and adjoint parameters) until previous data point or initial time
* Step 4: if not initial time go to step 2, if initial go to step 5
* Step 5: add the contribution of initial conditions to adjoint of parameters
* Step 6: read gradient


# Worked example: logistic function

## About the logistic (growth) function

- Simple population growth model with saturation
- 1-D ODE
- Three parameters 
- Known analytical solution
- Non-linear

## ODE Form {.smaller}

The ODE form of the logistic function is:

$$
\begin{cases}
\begin{align*}
\frac{dN}{dt} &= rN\left(1 - \frac{N}{K}\right) \\
N(0) &= N_{0}
\end{align*}
\end{cases}
$$

where:

- $K$ is the carrying capacity or maximum value the function can reach
- $r$ is the growth rate or steepness of the curve
- $N_{0}$ is the initial condition of the ODE 

## Equation form and observation model

- The equation form of the logistic function is given by:
$$N(t) = \frac{K}{1 + \left ( \frac{K}{N_{0}}-1 \right ) e^{-rt}}$$
- We can also add an observation model
$$N^{obs}_{t} \sim \mathcal{N}(N(t), \sigma),\; t \in T_{obs}=\{ t_{1},...,t_{n_{obs}} \}$$

## odin model

very compact and easy to read
```{r}
r_output <- function(path, highlight = NULL) {
  if (is.null(highlight)) {
    prefix <- "```r"
  } else {
    prefix <- sprintf('```{.r code-line-numbers="%s"}', highlight)
  }
  writeLines(c(prefix, readLines(path), "```"))
}
set.seed(1) # always the same
knitr::knit_hooks$set(small_margins = function(before, options, envir) {
  if (before) {
    par(mar = c(4, 4, .1, .1))
  }
})
```

```{r}
#| results: "asis"
r_output("models/logistic_growth_normal_obs.R")
```

The model (including noisy observation) has 4 parameters $r$, $K$, $N_{0}$ and $\sigma$ the s.d. of the observation process

## Logistic function plot

We can use odin.dust to integrate the ODE:

```{r}
#| echo: TRUE
generator <- odin.dust::odin_dust("models/logistic_growth_normal_obs.R")
```

## Logistic function plot

```{r}
#default parameters
N0 <- 2
K <- 100
r <- 1

mod <- generator$new(pars=list(r=r, N0=N0, K=K, sd_noise=5), time = 0, n_particles = 1)

n_obs <- 20
t_obs <- seq(1, n_obs)

#Plot the trajectory
tt <- seq(0, 25, length.out = 101)
y <- mod$simulate(tt)[,1,]
plot(tt, y, xlab = "t", ylab = "N(t)", main = "", type="l", ylim= c(0,120), lty=1)
```

## Logistic function plot + observations

```{r plot-observations}
n_obs <- 20
t_obs <- seq(1, n_obs)

#computes the mid-point time (t0 such that N(t0)=K/2)
t0 <- log(K/N0-1)/r

#plot the analytical solution of the plotted model
N_obs <- K/(1+exp(-r*(t_obs-t0)))
#points(t_obs, N_obs)

#generates noisy observations/data
sd_noise <- 5
d_df <- data.frame(time = t_obs,
                   observed = rnorm(N_obs,N_obs, sd_noise))
plot(tt, y, xlab = "t", ylab = "N(t)", main = "", type="l", ylim= c(0,120), lty=3)
points(t_obs, N_obs, pch=19, col="grey")
points(t_obs, d_df$observed, pch=19, col="black")
for(i in seq_along(t_obs)){
  lines(c(t_obs[i],t_obs[i]), c(N_obs[i],d_df$observed[i]))
}
```

## Gradient of log-likelihood (1) {.smaller}

The log-likelihood of the model is:
$$\log(L) = \sum_{t \in T_{obs}} \log \left( f(N^{obs}_{t}, N(t), \sigma ) \right )$$
with

- $N^{obs}_{t},t \in T_{obs}$ the observations
- $f$ the pdf of the Normal distribution

$\frac{\partial \log(L)}{\partial \sigma}$ can be calculated relatively easily and 
for partial derivatives wrt parameter $X \in \{ r,K,N_{0} \}$, we can use:
$$\frac{\partial \log(L)}{\partial X} = \sum_{t \in T_{obs}} \frac{\partial \log (f)}{\partial N} \bigg|_{N^{obs}_{t},N(t), \sigma} \frac{\partial N}{\partial X} \bigg|_{r,K,N_{0},t}$$

## Gradient of log-likelihood (2) {.smaller}

The log of the normal pdf is
$$ \log (f) = -\log \sigma - \frac{1}{2} \log (2\pi) - \frac{1}{2 \sigma^2} ( N^{obs}-N )^2$$
So we have

$$\frac{\partial \log(L)} {\partial \sigma} = - \sum_{t \in T_{obs}} \left ( \frac{1}{\sigma} +\frac{1}{\sigma^3}(N^{obs}_{t}-N)^2 \right ) $$

and we also get 
$$\frac{\partial \log (f)}{\partial N} \bigg|_{N(t), \sigma} = \frac{N^{obs}-N}{\sigma^2}$$ 

## Gradient of log-likelihood (3) {.smaller}

So for $X \in \{ r,K,N_{0} \}$ we finally get
$$\frac{\partial \log(L)}{\partial X} = \sum_{t \in T_{obs}} \frac{N^{obs}_{t}-N}{\sigma^2} \frac{\partial N}{\partial X} \bigg|_{r,K,N_{0},t}$$

To which we need to plug the partial derivatives with respect to $r$, $K$, $N_{0}$

$$
\begin{cases}
\begin{align*}
\frac{{\partial N(t)}}{{\partial r}} & = \frac{{K \cdot t \cdot (K - N_0) \cdot e^{-rt}}}{{N_0 \cdot \left(1 + \left ( \frac{K}{N_{0}}-1 \right ) \cdot e^{-rt}\right)^2}} \\
\frac{{\partial N(t)}}{{\partial K}}  & = \frac{1}{{\left(1 + \left ( \frac{K}{N_{0}}-1 \right ) \cdot e^{-rt}\right)^2}} \\
\frac{{\partial N(t)}}{{\partial N_0}} & = - \frac{{K \cdot e^{-rt}}}{{N_0^2 \cdot \left(1 + \left(\frac{{K}}{{N_0}}-1\right) \cdot e^{-rt}\right)^2}}
\end{align*}
\end{cases}
$$

## We can get R code for these

```{r, eval = FALSE}
#| echo: TRUE
# Gradient function with respect to K
partial_derivative_K <- function(K, N0, r, t) {
  gradient <- 1 / (1 + ((K - N0) / N0) * exp(-r * t))^2
  return(gradient)
}

# Gradient function with respect to N0
partial_derivative_N0 <- function(K, N0, r, t) {
  gradient <- (-K / N0^2) * exp(-r * t) / (1 + ((K - N0) / N0) * exp(-r * t))^2
  return(gradient)
}

# Gradient function with respect to r
partial_derivative_r <- function(K, N0, r, t) {
  gradient <- (K * t * (K - N0) * exp(-r * t)) / (N0 * (1 + ((K - N0) / N0) * exp(-r * t))^2)
  return(gradient)
}
```

```{r}
# Gradient function with respect to K
partial_derivative_K <- function(K, N0, r, t) {
  gradient <- 1 / (1 + ((K - N0) / N0) * exp(-r * t))^2
  return(gradient)
}

# Gradient function with respect to N0
partial_derivative_N0 <- function(K, N0, r, t) {
  gradient <- (-K / N0^2) * exp(-r * t) / (1 + ((K - N0) / N0) * exp(-r * t))^2
  return(gradient)
}

# Gradient function with respect to r
partial_derivative_r <- function(K, N0, r, t) {
  gradient <- (K * t * (K - N0) * exp(-r * t)) / (N0 * (1 + ((K - N0) / N0) * exp(-r * t))^2)
  return(gradient)
}
```

## and R code for the full likelihood gradient

```{r, eval = FALSE}
#| echo: TRUE
likelihood_gradient <- function(K, N0, r, sigma, N_obs, t_obs){
  t0 <- log(K/N0-1)/r
  g_LL_K <- 0
  g_LL_N0 <- 0
  g_LL_r <- 0
  g_LL_sigma <- 0
  for(i in seq_along(t_obs)){
    N_t <- K/(1+exp(-r*(t_obs[i]-t0)))
    
    #Calculate the simpler PD wrt sigma
    g_LL_sigma <- g_LL_sigma + 1/sigma + 1/sigma^3*(N_obs[i]- N_t)^2
    
    #Normal pdf contribution
    dlogf_dN <- (N_obs[i]- N_t)/sigma^2
    g_LL_K <- g_LL_K + dlogf_dN * partial_derivative_K(K, N0, r, t_obs[i])
    g_LL_N0 <- g_LL_N0 + dlogf_dN * partial_derivative_N0(K, N0, r, t_obs[i])
    g_LL_r <- g_LL_r + dlogf_dN * partial_derivative_r(K, N0, r, t_obs[i])
  
  }
  return(list(sigma = g_LL_sigma, K = g_LL_K, g_LL_N0, g_LL_r))
}
```

```{r}
likelihood_gradient <- function(K, N0, r, sigma, N_obs, t_obs){
  #browser()
  t0 <- log(K/N0-1)/r
  g_LL_K <- 0
  g_LL_N0 <- 0
  g_LL_r <- 0
  g_LL_sigma <- 0
  for(i in seq_along(t_obs)){
    N_t <- K/(1+exp(-r*(t_obs[i]-t0)))
    
    #Calculate the simpler PD wrt sigma
    g_LL_sigma <- g_LL_sigma + 1/sigma + 1/sigma^3*(N_obs[i]- N_t)^2
    
    #Normal pdf contribution
    dlogf_dN <- (N_obs[i]- N_t)/sigma^2
    g_LL_K <- g_LL_K + dlogf_dN * partial_derivative_K(K, N0, r, t_obs[i])
    g_LL_N0 <- g_LL_N0 + dlogf_dN * partial_derivative_N0(K, N0, r, t_obs[i])
    g_LL_r <- g_LL_r + dlogf_dN * partial_derivative_r(K, N0, r, t_obs[i])
  
  }
  return(list(sigma = g_LL_sigma, K = g_LL_K, N0 = g_LL_N0, r = g_LL_r))
}
```

## Gradient of log-likelihood

We this get painfully a function giving the gradient of the log-likelihood of our model:
```{r}
#| echo: TRUE
likelihood_gradient(K, N0, r, sd_noise, d_df$observed, t_obs)
```

Note that the trick work only because we had an exact solution of the ode!

## Gradient using autodiff

```{r}
#Run system until last observation
last_obs <- length(t_obs)
mod$initialize(pars=list(r=1, sd_noise=5), time = 0, n_particles = 1)
d <- dust::dust_data(d_df)
mod$set_data(d)
y_end <- mod$run(last_obs)

#reversing the ode's
generator_reverse <- odin.dust::odin_dust("models/reverse_AD_logistic.R")

N_curr <- N_obs[last_obs]
t_curr <- t_obs[last_obs]
adj_N_curr <- 0
adj_K_curr <- 0
adj_r_curr <- 0
y_curr <- y_end

reverse_mod <- generator_reverse$new(pars= list(r=1,
                                                N_end=N_curr,
                                                t_end=t_curr,
                                                adj_N_end=adj_N_curr,
                                                adj_K_end=0,
                                                adj_r_end=0), time=0, n_particles = 1)

contribution_data <- function(observed, model, sd){
  (observed-model)/sd^2
}

for(i in seq_along(t_obs)[-n_obs]){
  adj_N_curr <- adj_N_curr + contribution_data(N_curr, y_curr, sd_noise)
  reverse_mod$initialize(pars= list(r=1,
                                    N_end=N_curr,
                                    t_end=t_curr,
                                    adj_N_end=adj_N_curr,
                                    adj_K_end=adj_K_curr,
                                    adj_r_end=adj_r_curr), time=0, n_particles = 1)
  reverse_mod$run(t_curr-t_obs[n_obs-i])

  N_curr <- reverse_mod$info()$index$N
  t_curr <- t_obs[n_obs-i]
  adj_N_curr <- reverse_mod$info()$index$adj_N
  adj_K_curr <- reverse_mod$info()$index$adj_K
  adj_r_curr <- reverse_mod$info()$index$adj_r
}
```

## Gradient using autodiff
```{r}
#| echo: TRUE
reverse_mod$state()[adj_N_curr] 
reverse_mod$state()[adj_K_curr]
reverse_mod$state()[adj_r_curr]
```

## Gradient using numerical differentiation


## Running Gradient descent


<!--  LocalWords:  revealjs sprintf writeLines readLines 
 -->
