---
title: "Hamiltonian Monte Carlo, AutoDiff and odin"
author: "Marc Baguelin"
format:
  revealjs:
    footer: "[HMC, AutoDiff and odin](https://www.imperial.ac.uk/people/m.baguelin)"
    slide-number: c/t
bibliography: papers_autodiff.bib
---

# Principles

## Hamiltonian Monte Carlo (HMC)

- for more details see e.g. @neal_mcmc_2011 or @betancourt_conceptual_2018
- an algorithm akin to RWMH-MCMC which exploits Hamiltonian dynamics to move more efficiently in the parameter space
- requires the gradient of the log-posterior density (the score)
- In its simplest form, uses two parameters, $\epsilon$ (small) and $L$ (integer)

## Hamiltonian dynamics - ideas

- We augment $\theta$ (position in the parameter space) with a "momentum" $r$

- While the dimension of the space to explore is doubled, this actually helps 

- We can now describe the system in term of its energy called "Hamiltonian", decomposed into kinetic (only depends on momentum) and potential (only depends on position)
$$
H(\theta, r) = T(r) + U(\theta)
$$

## Kinetic and potential energies

Assuming a mass matrix to be identity, we get the following expressions:

- Kinetic energy
$$
T(r) = \frac{r^\intercal r}{2}
$$

- Potential energy
$$
U(\theta) = - \log [ \pi(\theta) L(\theta|D)]
$$

## Hamiltonian dynamics - equations

We can move at constant energy levels using the Hamiltonian dynamics equations:
\begin{cases}
\begin{align*}
\frac{d\theta}{dt} &= \frac{\partial H}{\partial r} \\
\frac{dr}{dt} &= -\frac{\partial H}{\partial \theta}
\end{align*}
\end{cases}

with $H$ the Hamiltonian of the system

## Leapfrog integration

- We can thus move at almost constant energy level using the St√∂rmer-Verlet ("leapfrog") integrator
\begin{cases}
\begin{align}
r_{t+\frac{1}{2}} &= r_t - \frac{1}{2} \nabla U(\theta_t) \cdot \epsilon \\
\theta_{t+1} &= \theta_t + r_{t+\frac{1}{2}} \cdot \epsilon \\
p_{t+1} &= p_{t+\frac{1}{2}} - \frac{1}{2} \nabla U(q_{t+1}) \cdot \epsilon 
\end{align}
\end{cases}

- More stable than Euler integration

## Algorithm

- Step 1: draw $r$ from a multivariate normal
- Step 2: run $L$ leapfrog step from $(\theta, r)$ to reach $(\theta^*,r^*)$
- Step 3: accept or reject with probability
$$\min[1, e^{H(\theta,r) - H(\theta^*,r^*)}]$$
- Step 4: run 1 & 2 until a certain number of steps is reached

N.B. As the Leapfrog integration keep the Hamiltonian almost constant the acceptance rate is high even for relatively big steps $\epsilon$

## AutoDiff

- Key quantity in the algorithm is the gradient of the log-posterior density $\nabla U(\theta)= \nabla \{ - \log [ \pi(\theta) L(\theta|D)] \}$
- AutoDiff provides fast methods for getting gradient of log-likelihood of odin model

## Inference space vs odin model space

- odin model parameters can live on constrained space
- HMC needs differential space and is easier if space is unconstrained
- e.g. in [stan](https://mc-stan.org/docs/reference-manual/variable-transforms.html) inference is done on an unconstrained space 
- similar problem than the problem of the propagation of the gradient from the odin model through the transform function

# Worked example: SIR model

```{r}
r_output <- function(path, highlight = NULL) {
  if (is.null(highlight)) {
    prefix <- "```r"
  } else {
    prefix <- sprintf('```{.r code-line-numbers="%s"}', highlight)
  }
  writeLines(c(prefix, readLines(path), "```"))
}
set.seed(1) # always the same
knitr::knit_hooks$set(small_margins = function(before, options, envir) {
  if (before) {
    par(mar = c(4, 4, .1, .1))
  }
})
```

## odin interface for AutoDiff

- specify which parameter to differentiate

```{r, eval=FALSE}
#| echo: TRUE
beta <- user(0.2, differentiate = TRUE)
gamma <- user(0.1, differentiate = TRUE)
I0 <- user(10, differentiate = TRUE)
```

- note that we might not want to differentiate all the parameters of the odin model
  - e.g. data input
  - or discrete parameters

## model odin code

an SIR model with poisson observation
```{r odin-SIR-with-observations}
#| results: "asis"
r_output("models/sir_adjoint.R")
```

## Compiling the model

create generator for dust model
```{r non-eval-model-code}
#| echo: TRUE
gen <- odin.dust::odin_dust("models/sir_adjoint.R")
```

## Set up data & model

Read the data and put them in the right format
```{r}
#| echo: TRUE
incidence <- read.csv("data/incidence.csv")
incidence <- data.frame(
  time = incidence$day * 4,
  cases_observed = incidence$cases)
d <- dust::dust_data(incidence)
```

Generate a set of parameters
```{r}
#| echo: TRUE
# Create set of parameters
pars <- list(beta = 0.25, gamma = 0.1, I0 = 1)
```

Creates a new dust model and attach the data to it
```{r}
#| echo: TRUE
# Create new deterministic model with data attached
mod <- gen$new(pars, 0, 1, deterministic = TRUE)
mod$set_data(d)
```

## Unconstrained parameter space

- We use log to map the parameters and the gradient from $\mathbb{R}_{+}^{d}$  to the $\mathbb{R}^{d}$ "inference" space

- We use reversely exponential to move back gradient and parameters in the "modelling" space

```{r transform-functions}
#| echo: TRUE
g <- function(theta) {as.list(exp(theta))}
dg <- function(theta) {exp(theta)}
```

## Gradient function

A wrapper to transform the parameters, run adjoints and scale gradient back
```{r compute-gradient-function}
#| echo: TRUE
compute_gradient <- function(mod, theta, trans, pd_trans, t = 0){
  pars <- trans(theta)
  mod$update_state(pars, time = t)
  res_adj <- mod$run_adjoint()
  list(log_likelihood = -res_adj$log_likelihood,
       gradient = -pd_trans(theta)*res_adj$gradient)
}
```

## HMC step

```{r}
#| echo: TRUE
HMC_step <- function(mod, current_theta, epsilon, L, g, dg){
  current_v <- rnorm(length(theta),0,1) # independent standard normal variates
  theta <- current_theta
  v <- current_v
  
  # Make a half step for momentum at the beginning
  v <- v - epsilon * compute_gradient(mod, theta, g, dg)$gradient / 2
  
  # Alternate full steps for position and momentum
  for (i in 1:L)
  {
    # Make a full step for the position
    theta <- theta + epsilon * v
    # Make a full step for the momentum, except at end of trajectory
    if (i!=L) v <- v - epsilon * compute_gradient(mod, theta, g, dg)$gradient   }
  # Make a half step for momentum at the end.
  v <- v - epsilon * compute_gradient(mod, theta, g, dg)$gradient / 2
  # Negate momentum at end of trajectory to make the proposal symmetric
  v <- -v
  # Evaluate potential and kinetic energies at start and end of trajectory
  current_U <- compute_gradient(mod, current_theta, g, dg)$log_likelihood
  current_K <- sum(current_v^2) / 2
  proposed_U <- compute_gradient(mod, theta, g, dg)$log_likelihood
  proposed_K <- sum(v^2) / 2
  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
  {
    return (theta) # accept
  } else {
    return (current_theta) # reject
  }
}
```

## Run the HMC

- Log transform the epidemiological parameters
```{r}
#| echo: TRUE
theta <- log(unlist(pars))
```

- Run the HMC chain
```{r}
#| echo: TRUE
theta <- log(unlist(pars))
n_steps <- 10000
theta_chain <- NULL
for(i in seq(n_steps)){
  theta <- HMC_step(mod, theta, 0.015, 10, g, dg)
  theta_chain <- rbind(theta_chain, c(theta,compute_gradient(mod, theta, g, dg)$log_likelihood))
}
```

## Plot the resulting chain

```{r}
#| echo: TRUE
plot(exp(theta_chain[,"gamma"]), type="l")
```

## Further work

- $\epsilon$ and $L$ can be defined adaptively using NUTS @hoffman_no-u-turn_2014
- the Kinetic energy can be tuned in order to optimise the exploration of the parameter space @livingstone_kinetic_2019
- the geometry of the distribution space can be exploited using Riemannian manifolds @Girolami2011

## References


